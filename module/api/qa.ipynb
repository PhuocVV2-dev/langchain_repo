{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d0a1c8",
   "metadata": {},
   "source": [
    "# Document Question Answering\n",
    "\n",
    "An example of using Chroma DB and LangChain to do question answering over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652985d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe5351",
   "metadata": {},
   "source": [
    "## Load documents\n",
    "\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b352847",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('state_of_the_union.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478be861",
   "metadata": {},
   "source": [
    "## Split documents\n",
    "\n",
    "Split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd1adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b30aff",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB\n",
    "\n",
    "Create embeddings for each chunk and insert into the Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d2a049",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\aws\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:81\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m openai_api_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msk-C6uSLwep0Yy1q6zDEVFnT3BlbkFJCEjfMlPTMNj0WHDMrfpr\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39mopenai_api_key)\n\u001b[1;32m----> 4\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_documents(texts, embeddings)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\aws\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:603\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    602\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 603\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(\n\u001b[0;32m    604\u001b[0m     texts\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m    605\u001b[0m     embedding\u001b[39m=\u001b[39membedding,\n\u001b[0;32m    606\u001b[0m     metadatas\u001b[39m=\u001b[39mmetadatas,\n\u001b[0;32m    607\u001b[0m     ids\u001b[39m=\u001b[39mids,\n\u001b[0;32m    608\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[0;32m    609\u001b[0m     persist_directory\u001b[39m=\u001b[39mpersist_directory,\n\u001b[0;32m    610\u001b[0m     client_settings\u001b[39m=\u001b[39mclient_settings,\n\u001b[0;32m    611\u001b[0m     client\u001b[39m=\u001b[39mclient,\n\u001b[0;32m    612\u001b[0m     collection_metadata\u001b[39m=\u001b[39mcollection_metadata,\n\u001b[0;32m    613\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    614\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\aws\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:558\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    526\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[0;32m    527\u001b[0m     \u001b[39mcls\u001b[39m: Type[Chroma],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    538\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Chroma:\n\u001b[0;32m    539\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \n\u001b[0;32m    541\u001b[0m \u001b[39m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[39m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m     chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    559\u001b[0m         collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[0;32m    560\u001b[0m         embedding_function\u001b[39m=\u001b[39membedding,\n\u001b[0;32m    561\u001b[0m         persist_directory\u001b[39m=\u001b[39mpersist_directory,\n\u001b[0;32m    562\u001b[0m         client_settings\u001b[39m=\u001b[39mclient_settings,\n\u001b[0;32m    563\u001b[0m         client\u001b[39m=\u001b[39mclient,\n\u001b[0;32m    564\u001b[0m         collection_metadata\u001b[39m=\u001b[39mcollection_metadata,\n\u001b[0;32m    565\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m     chroma_collection\u001b[39m.\u001b[39madd_texts(texts\u001b[39m=\u001b[39mtexts, metadatas\u001b[39m=\u001b[39mmetadatas, ids\u001b[39m=\u001b[39mids)\n\u001b[0;32m    568\u001b[0m     \u001b[39mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\aws\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:84\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import chromadb python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install chromadb`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     89\u001b[0m \u001b[39mif\u001b[39;00m client \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client_settings \u001b[39m=\u001b[39m client_settings\n",
      "\u001b[1;31mValueError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "openai_api_key = \"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\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectordb = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb7866",
   "metadata": {},
   "source": [
    "## Create the chain\n",
    "\n",
    "Initialize the chain we will use for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35164427",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff96efe",
   "metadata": {},
   "source": [
    "## Ask questions!\n",
    "\n",
    "Now we can use the chain to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5851c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The president said that Ketanji Brown Jackson is one of the nation's top legal minds and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. He also said that she is a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He described her as a consensus builder.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf9016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
